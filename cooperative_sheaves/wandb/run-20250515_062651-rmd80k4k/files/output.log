  0%|                                                                                                              | 0/10 [00:00<?, ?it/s]/home/alunos/Dados/.pyenv/versions/hsnn/lib/python3.11/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
Baseline MSE: 1.0362 +/- 0.4085
Fold 0 | Epochs: 499 | Best epoch: 419
Best train loss: 0.0594 | Best test loss: 2.0133
  warnings.warn(out)
 40%|████████████████████████████████████████▍                                                            | 4/10 [12:40<19:00, 190.09s/it]
Baseline MSE: 0.9335 +/- 0.3795
Fold 1 | Epochs: 499 | Best epoch: 421
Best train loss: 0.0572 | Best test loss: 1.7893
Baseline MSE: 0.9920 +/- 0.4016
Fold 2 | Epochs: 499 | Best epoch: 432
Best train loss: 0.0581 | Best test loss: 1.7835
Baseline MSE: 1.0614 +/- 0.4060
Fold 3 | Epochs: 499 | Best epoch: 339
Best train loss: 0.0460 | Best test loss: 1.8861
Baseline MSE: 1.0146 +/- 0.4514
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/alunos/Dados/andre/cooperative-sheaves/neural-sheaf-diffusion/cooperative_sheaves/exp/run_synth.py", line 154, in <module>
    train_loss, best_test_loss = run_exp(wandb.config, train_loader, test_loader, fold)
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/alunos/Dados/andre/cooperative-sheaves/neural-sheaf-diffusion/cooperative_sheaves/exp/run_synth.py", line 57, in run_exp
    train_loss = train(model, optimizer, train_data)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/alunos/Dados/andre/cooperative-sheaves/neural-sheaf-diffusion/cooperative_sheaves/exp/run_synth.py", line 25, in train
    optimizer.step()
  File "/home/alunos/Dados/.pyenv/versions/hsnn/lib/python3.11/site-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/home/alunos/Dados/.pyenv/versions/hsnn/lib/python3.11/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/alunos/Dados/.pyenv/versions/hsnn/lib/python3.11/site-packages/torch/optim/adam.py", line 223, in step
    adam(
  File "/home/alunos/Dados/.pyenv/versions/hsnn/lib/python3.11/site-packages/torch/optim/optimizer.py", line 154, in maybe_fallback
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/alunos/Dados/.pyenv/versions/hsnn/lib/python3.11/site-packages/torch/optim/adam.py", line 784, in adam
    func(
  File "/home/alunos/Dados/.pyenv/versions/hsnn/lib/python3.11/site-packages/torch/optim/adam.py", line 543, in _multi_tensor_adam
    torch._foreach_addcmul_(
KeyboardInterrupt
